# datasetå¤„ç†
# ä¸»è¦ä½œç”¨ï¼šå°†éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºmelså¹¶å­˜å‚¨åœ¨tensorä¸­ï¼Œæ–¹ä¾¿åç»­æ¨¡å‹è¾“å…¥
# torch.__version__
# Out[4]: '2.2.2+cpu'
# numpy.__version__
# Out[6]: '2.0.2'
# import torchæ—¶æŠ¥é”™
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import glob
import librosa
from utils.audio_tools import gvad
import numpy as np
import os
from collections import defaultdict


class RAVDESSMelS(Dataset):
    """
    override the RAVDESS dataset and make it suitable for the mel spectrogram process
    """
    def __init__(self, dataset_dir, mel_specs_kwargs=None, speech_list=None, out_dir=None):
        if mel_specs_kwargs is None:
            mel_specs_kwargs = {}
        self.dataset_dir = dataset_dir
        self.mel_specs_kwargs = mel_specs_kwargs if mel_specs_kwargs else {}

        self.out_dir = out_dir

        if speech_list is None:
            self.speech_list = glob.glob(f"{dataset_dir}*/*.wav")
        else:
            self.speech_list = speech_list

    def __getitem__(self, index):
        fpath = self.speech_list[index]
        fn = fpath.split("/")[-1].split(".")[0]

        sig, sr = librosa.load(self.speech_list[index])

        features = ['modality', 'vocal_channel', 'emotion', 'emotion_intensity', 'statement', 'repetition', 'actor']
        info = {f: c for c, f in zip(fn.split("-"), features)}
        mel_spec = truncate_melspecs(sig, return_va_point=False, sr=sr, mel_specs_kwargs=self.mel_specs_kwargs)
        return torch.Tensor(mel_spec), sr, info, fn

    def __len__(self):
        return len(self.speech_list)


def truncate_melspecs(sig, return_va_point=False, sr=22050, mel_specs_kwargs=None):
    """
    Convert the spectrogram to an amplitude map
    and intercept the main part with the gvad function
    """
    if mel_specs_kwargs is None:
        mel_specs_kwargs = {}
    mel_spec = librosa.power_to_db(librosa.feature.melspectrogram(y=sig, sr=sr, **mel_specs_kwargs))
    va_point = gvad(librosa.db_to_amplitude(mel_spec) ** 2)  # å°†dbè°±å›¾è½¬ä¸ºæ™®é€šæŒ¯å¹…è°±å›¾
    # ç”¨VADæ£€æµ‹åˆ°çš„æœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µèµ·å§‹å¸§å’Œç»“æŸå¸§ç”¨åœ¨æˆªå–mel
    mel_spec = mel_spec[:, va_point[0]:va_point[1]]
    if return_va_point:
        return mel_spec, va_point
    else:
        return mel_spec


emotion_dict = {
    "01": "neutral",
    "02": "calm",
    "03": "happy",
    "04": "sad",
    "05": "angry",
    "06": "fearful",
    "07": "disgust",
    "08": "surprised"
}


class RAVDESSMultimodalDataset(Dataset):
    """
    æ–°ç‰ˆæœ¬çš„æ•°æ®é›†å¤„ç†ï¼Œå°†éŸ³é¢‘å’Œè§†é¢‘çš„å¤„ç†ç»Ÿä¸€å°è£…åœ¨ä¸€ä¸ªclass
    å¤šæ¨¡æ€æ•°æ®é›†ï¼šä» .wav æå– Mel é¢‘è°±å›¾ï¼ŒåŒæ—¶åŠ è½½å·²é¢„å¤„ç†å¥½çš„äººè„¸å¸§ï¼ˆ.npyï¼‰
    ä½ çš„è§†é¢‘é¢„å¤„ç†è„šæœ¬åº”å·²ç”Ÿæˆäº† _facecropped.npy æ–‡ä»¶ï¼ˆæ¯æ¡æ ·æœ¬çº¦ 15 å¸§ï¼‰
    """
    def __init__(self, dataset_dir, mel_specs_kwargs=None, frames=15, transform=None, verbose=True, debug=False):
        self.dataset_dir = dataset_dir
        self.mel_specs_kwargs = mel_specs_kwargs if mel_specs_kwargs else {}
        self.frames = frames
        self.transform = transform
        self.verbose = verbose  # å¢åŠ ä¸€ä¸ªå¼€å…³ï¼Œæ‰“å°ç¼ºå¤±æ–‡ä»¶
        self.debug = debug

        # RAVDESS æ ‡ç­¾æ˜ å°„è¡¨ï¼ˆæƒ…ç»ªç¼–å· â†’ labelï¼‰
        self.label_map = {k: int(k) for k in emotion_dict.keys()}  # 01 -> 1, ..., 08 -> 8

        # è·å–æ‰€æœ‰ .wav æ–‡ä»¶è·¯å¾„ï¼ˆéŸ³é¢‘ä¸»æ§ï¼‰
        self.wav_paths = []
        for root, _, files in os.walk(dataset_dir):
            for f in files:
                if f.endswith(".wav") and f.startswith("03"):
                    self.wav_paths.append(os.path.join(root, f))

    def __len__(self):
        return len(self.wav_paths)

    def __getitem__(self, idx):
        wav_path = self.wav_paths[idx]
        base_name = os.path.splitext(os.path.basename(wav_path))[0]
        label_id = base_name.split("-")[2]
        label = torch.tensor(self.label_map[label_id], dtype=torch.long)
        label_name = emotion_dict[label_id]

        # === éŸ³é¢‘ç‰¹å¾æå–ï¼ˆaudio-onlyï¼‰ ===
        try:
            sig, sr = librosa.load(wav_path, sr=22050)
            mel_spec = librosa.power_to_db(
                librosa.feature.melspectrogram(y=sig, sr=sr, **self.mel_specs_kwargs)
            )
            va = gvad(librosa.db_to_amplitude(mel_spec) ** 2)
            mel_spec = mel_spec[:, va[0]:va[1]]

            max_len = 128  # ç»Ÿä¸€æ—¶é—´é•¿åº¦ï¼Œåç»­å¯æ­é…self.n_melså’Œself.max_lenå˜ä¸ºå¯è°ƒæ•´çš„å‚æ•°
            if mel_spec.shape[1] > max_len:
                mel_spec = mel_spec[:, :max_len]
            else:
                pad_width = max_len - mel_spec.shape[1]
                mel_spec = np.pad(mel_spec, ((0, 0), (0, pad_width)), mode='constant')

            audio_feat = torch.tensor(mel_spec, dtype=torch.float32)  # shape: [128, T]
        except Exception as e:
            if self.verbose:
                print(f"[AUDIO ERROR] Failed to process {wav_path}: {e}")

            #audio_feat = torch.zeros(self.n_mels, self.max_len)
            audio_feat = torch.zeros(128, 128)

        # === è§†é¢‘æ¨¡æ€ï¼šé€šè¿‡åç¼€éƒ¨åˆ†åŒ¹é…å¯¹åº”çš„.npy ===
        # æ›´æ­£äº†åœ¨æŸ¥æ‰¾è§†é¢‘æ¨¡æ€æ–‡ä»¶æ—¶çš„å‘½åé—®é¢˜ï¼Œä¹‹å‰æ˜¯é€šè¿‡wavæ–‡ä»¶æŸ¥æ‰¾ï¼Œä½†æ˜¯å¿½ç•¥äº†å› æ¨¡æ€ä¸åŒ
        # è€Œå‡ºç°çš„åç§°æ›¿æ¢é—®é¢˜ï¼Œä»…éŸ³é¢‘ä»¥03å¼€å¤´ï¼Œä»…è§†é¢‘ä»¥02å¼€å¤´
        video_suffix = "-".join(base_name.split("-")[1:])
        actor_dir = os.path.dirname(wav_path)
        candidate_npy_files = [f for f in os.listdir(actor_dir) if f.startswith("02-") and f.endswith("_facecropped.npy")]

        npy_path = None
        for fname in candidate_npy_files:
            if video_suffix in fname:
                npy_path = os.path.join(actor_dir, fname)
                break

        if npy_path is None or not os.path.exists(npy_path):
            if self.verbose:
                print(f"[VIDEO WARNING] Missing .npy file for: {base_name}")
            # è‹¥æ‰¾ä¸åˆ°å¯¹åº”è§†é¢‘ï¼Œä½¿ç”¨å…¨ 0 å ä½å¸§
            video_faces = torch.zeros(self.frames, 3, 224, 224)
        else:
            try:
                faces_np = np.load(npy_path)  # shape: [T, 224, 224, 3]
                faces_np = faces_np[:self.frames]  # è£å‰ª/é™åˆ¶å¸§æ•°
                faces = torch.tensor(faces_np, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0  # [T, 3, 224, 224]
                if self.transform:
                    faces = self.transform(faces)
                video_faces = faces
            except Exception as e:
                if self.verbose:
                    print(f"[VIDEO ERROR] Failed to load {npy_path}: {e}")
                video_faces = torch.zeros(self.frames, 3, 224, 224)

        if self.debug:
            # åœ¨trainçš„æ—¶å€™å‡ºç°IndexError: Target 8 is out of bounds
            # æ„å‘³ç€ä¼ ç»™CrossEntropyLossçš„labelsä¸­ï¼Œå‡ºç°äº†å€¼ä¸º 8 çš„æ ‡ç­¾
            # éœ€è¦å°†æ ‡ç­¾ç¼–å·åš-1å¤„ç†
            # label -> int(label_id) - 1
            return audio_feat, video_faces, int(label_id) - 1, label_name
        else:
            return audio_feat, video_faces, int(label_id) - 1

    def stats_per_actor(self):
        """
        ç»Ÿè®¡æ¯ä¸ªæ¼”å‘˜çš„æ ·æœ¬æ•°é‡
        """
        counts = defaultdict(int)  # æ¯æ¬¡è®¿é—®ä¸å­˜åœ¨çš„keyæ—¶ï¼Œä¸ä¼šæŠ¥é”™ï¼Œè€Œæ˜¯è‡ªåŠ¨åˆå§‹åŒ–ä¸º0
        for path in self.wav_paths:
            actor_folder = os.path.basename(os.path.dirname(path))  # Actor_01, Actor_02...
            counts[actor_folder] += 1

        print("\nğŸ“Š æ ·æœ¬ç»Ÿè®¡ï¼ˆæ¯ä½æ¼”å‘˜ï¼‰ï¼š")
        for actor, count in sorted(counts.items()):
            print(f"{actor}: {count} samples")


# #datasetä½¿ç”¨å®ä¾‹
# dataset = RAVDESSMultimodalDataset(
#     dataset_dir="F:/RAVDESS_dataset/Audio&Video_Speech_Actors_01-24/",
#     mel_specs_kwargs={"n_mels": 128, "n_fft": 2048, "hop_length": 512},
#     frames=15,
#     verbose=True  # æ‰“å°ç¼ºå¤±æ–‡ä»¶
# )

# # ç»Ÿè®¡æ¯ä½æ¼”å‘˜çš„æ ·æœ¬æ•°é‡ï¼ˆå¯é€‰ï¼‰
# dataset.stats_per_actor()