import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from dataset.dataset import RAVDESSMultimodalDataset
from models.audio_modelx import get_audio_model
from models.video_model import VideoModel
from models.fusion_model import FusionModel
import os
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# === é…ç½® ===
config = {
    "dataset_path": "F:/RAVDESS_dataset/Audio&Video_Speech_Actors_01-24",
    "batch_size": 8,
    "epochs": 30,
    "lr": 1e-4,
    "audio_dim": 256,
    "video_dim": 512,
    "num_classes": 8,
    "device": "cuda" if torch.cuda.is_available() else "cpu"
}

# === æ•°æ®é›†åŠ è½½ ===
dataset = RAVDESSMultimodalDataset(
    dataset_dir=config["dataset_path"],
    mel_specs_kwargs={"n_mels": 128, "n_fft": 2048, "hop_length": 512},
    frames=15,
    debug=False
)
dataloader = DataLoader(dataset, batch_size=config["batch_size"], shuffle=True)

# === æ„å»ºæ¨¡å‹ ===
audio_model = get_audio_model(mode="embedding", embedding_dim=config["audio_dim"])
video_model = VideoModel(hidden_dim=config["video_dim"] // 2)  # åŒå‘LSTMè¾“å‡ºç¿»å€
fusion_model = FusionModel(config["audio_dim"], config["video_dim"], config["num_classes"])

# å°†æ¨¡å‹ç§»è‡³è®¾å¤‡
audio_model.to(config["device"])
video_model.to(config["device"])
fusion_model.to(config["device"])

# === è®­ç»ƒç»„ä»¶ ===
criterion = nn.CrossEntropyLoss()
params = list(audio_model.parameters()) + list(video_model.parameters()) + list(fusion_model.parameters())
optimizer = optim.Adam(params, lr=config["lr"])

# === è®­ç»ƒå¾ªç¯ ===
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
for epoch in range(config["epochs"]):
    audio_model.train()
    video_model.train()
    fusion_model.train()

    total_loss, correct, total = 0, 0, 0

    for audio, video, labels in dataloader:
        audio = audio.to(config["device"])
        video = video.to(config["device"])
        labels = labels.to(config["device"])

        # å‰å‘ä¼ æ’­
        audio_feat = audio_model(audio)           # [B, D1]
        video_feat = video_model(video)           # [B, D2]
        logits = fusion_model(audio_feat, video_feat)  # [B, num_classes]

        # è®¡ç®—æŸå¤±ä¸ä¼˜åŒ–
        loss = criterion(logits, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç»Ÿè®¡ä¿¡æ¯
        total_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)

    acc = correct / total
    print(f"ğŸ“˜ Epoch {epoch+1}/{config['epochs']} | Loss: {total_loss:.4f} | Accuracy: {acc:.4f}")

# === ä¿å­˜æ¨¡å‹ ===
torch.save({
    "audio_model": audio_model.state_dict(),
    "video_model": video_model.state_dict(),
    "fusion_model": fusion_model.state_dict()
}, "multimodal_model_0419_1.pt")

print("âœ… æ¨¡å‹è®­ç»ƒå®Œæ¯•ï¼Œå·²ä¿å­˜ä¸º multimodal_model_0419_1.pt")
